<!DOCTYPE html>
<html>

<head>
    <title>Sadie Garner</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="author" content="Amy Pegram, 1825142">
    <meta name="description" content="Blog: `+ BlogTitle + `">
    <link rel="icon" href="/WSOA3028A_1825142/Logo.png" type="image/png">

    <meta property="og:title" content="Sadie Garner" />
    <meta property="og:type" content="website" />
    <meta property="og:url"
        content="https://vividcurlyfry.github.io/WSOA3028A_1825142/Blogs/Blog14-RRCriminalMachineLearning.html" />
    <meta property="og:image" content="WSOA3028A_1825142/Logo.png" />
    <meta property="og:description"
        content="Blog: Close Reading - Criminal machine learning - Carl Bergstrom and Jevin West">

    <link href="/WSOA3028A_1825142/css/stylesheet.css" rel="stylesheet">
    <script src="/WSOA3028A_1825142/js/blogtemplate.js" async></script>
    <script src="/WSOA3028A_1825142/js/navbar.js" async></script>
</head>

<body
    onload="LoadBlogPost('Close Reading - Criminal machine learning - Carl Bergstrom and Jevin West',` <p>Two engineering researchers (Xiaolin Wu and Xi Zhang) posted an article describing algorithms that have been developed to identify criminals. These algorithms label a face “criminal” or “non-criminal”.</p><h2>Cesare Lombroso's physiognomic criminology</h2><p>Cesare Lombroso claims that criminals are born as such. The doctor claims that criminals exhibit both physical and psychological features that make them criminals. Lombroso’s research was later debunked and based heavily on the racist ideas of the time.</p><h3>Wu and Zhang's (2016) approach</h3><p>Wu and Zhang revisited Lombroso’s racist ideas and attempted to use machine learning to bring them to life. The engineers claim that their programme has  a 90% accuracy rate at distinguishing criminals from non-criminals from a simple headshot. They claim that because the algorithm is a machine, and therefore inherently objective and not biased in any way. </p><h3>A biased training set?</h3><p>As mentioned in my previous blog post (HYPERLINK), there is in fact a lot of proof that facial recognition software is in fact biased. This paper reiterates that an algorithm can only be as non-biased as you programme it to be. Wu and Zhang used over 1,800 photos of Chinese criminals and non-criminals. The pictures of criminals are pictures of convicted criminals provided by police departments. The pictures of non-criminals are found on the World Wide Web</p><p>There are two massive problems with how these engineers chose their data set for the algorithm. The first being that the non-criminals have been able to choose how their photo was taken. These photos were taken from the World Wide Web, presumably where people posted photos for professional purposes. These photos would presumably show the person in a positive light. The photos that were used for criminals are presumably photos that would’ve not been chosen by the criminal. These photos are also made to not show the person in a positive light.</p><p>The second bias that is being shown in the selection of pictures is the photos are of CONVICTED criminals. This means that the algorithm could just as well be labelling criminals as people who are more likely to be convicted due to the way that they look. That is, a jury’s biases leads to the conviction of an innocent person, and this person’s photo is being used to represent a criminal to the algorithm.,<p>Therefore, the choice of data set causes the algorithm to be obviously racist.</p><h2>For the want of a smile</h2><p><img class=&quot pictureRow &quot src=&quot /WSOA3028A_1825142/Theory/SmilePic.png &quot alt= &quot Picture showing the angles used in the algorithm &quot height=300rem></p><p>“The algorithm finds that criminals have shorter distances d between the inner corners of the eyes, smaller angles θ between the nose and the corners of the mouth, and higher curvature ρ to the upper lip.” [1]</p><p>The authors state that there is an obvious reason that the nose-mouth angle θ and the curvature ρ. That is, when a person smiles, the corners of the lips spread out and the upper lip straightens.</p><p>It is obvious to see that people who are not smiling have been identified as criminals, as the pictures used for criminals are ID photos from the police. This means the non-criminal photos would include someone slightly smiling while the criminal photos would be of a person not smiling. The authors highlight that the algorithm analyses facial expressions as opposed to facial features.</p><p>Overall, the algorithm is identifying where the pictures have from. That is, whether that picture is from the police or a photo posted on the World Wide Web.</p><p>Zhang and Wu include photos that show faces the algorithm produced to represent criminals and non-criminals. The left are composite criminal faces and the right are composite non-criminal faces</p><p><img class=&quot pictureRow &quot src=&quot /WSOA3028A_1825142/Theory/SmileCompPic.png &quot alt= &quot Picture showing four different composite faces &quot height=500rem></p><p>The smiling theory the authors have stated is easily proven using these photos.</p><h2>Conclusion</h2><p>The authors state that they did not even need to analyse the algorithm to understand that the technology is biased and can easily debunked. It can be seen that a machine learning algorithm is only as good as it’s data set. This algorithm is discriminating based on context-specific cues. <h2>The COMPAS Recidivism Algorithm – A Real Life Example</h2><p>This is an Algorithm that rates the likelihood of a criminal committing a future crime. The algorithm gives a person a score, which is called a risk assessment. The result of these algorithms “are used to inform decisions about who can be set free at every stage of the criminal justice system, from assigning bond amounts to even more fundamental decisions about defendants’ freedom.” [2]</p><p> Based on the research I’ve done on these algorithms, it comes as no surprise that the results of this algorithm are biased [2]:  <ul><li>The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.</li><li>White defendants were mislabeled as low risk more often than black defendants.</li></ul></p><p>This is a direct example of algorithm’s such as Zhang and Wu’s being used by racist law makers to entrap minorities.</p><h2>Wrongly Accused by an Algorithm</h2> <p>Another real life example of algorithms like these being biased is Robert Julian-Borchak Williams story. This was a man who was questioned by police solely based on the fact that facial recognition software found a match between him and a burglar. When Williams got to the police station, they showed him the picture that was used for the facial recognition match. He held it up next to his face, it was very obviously not him.</p><p>He was held in custody for 30 hours before being released. The facial recognition technology was the only evidence the police had for holding Williams. They did not even ask him for an alibi, which he had, on film.</p><p>Facial recognition can be a privacy violation when correct, and a tool for perpetuating racism when incorrect.</p>`, `<p><cite>[1] C. B. a. J. West, \“Criminal machine learning,”\ 2017. [Online]. Available: <a href= &quot  https://callingbull.org/case_studies/case_study_criminal_machine_learning.html &quot> https://callingbull.org/case_studies/case_study_criminal_machine_learning.html</a> [Accessed 28 06 2020].</cite></p><p><cite>[2] J. Angwin, J. Larson, S. Matt and L. Kirchner, “Machine Bias,” 23 May 2016. [Online]. Available: <a href= &quot https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing &quot> https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a> [Accessed 28 06 2020].</p></cite>`,'/WSOA3028A_1825142/Blogs/Blog13-TikTokShadowBanning.html','/WSOA3028A_1825142/Blogs/Blog15-ExternalDependency.html'); generateNav()">
    <nav></nav>
</body>

</html>